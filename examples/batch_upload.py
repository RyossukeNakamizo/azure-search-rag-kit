"""
ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«

å¤§é‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åŠ¹ç‡çš„ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç™»éŒ²ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã€‚
ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã€åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã€ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è§£èª¬ã€‚

Usage:
    python examples/batch_upload.py
"""

import sys
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from dotenv import load_dotenv

from search_client import AzureSearchClient, create_document

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()


# ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
SAMPLE_DOCUMENTS = [
    {
        "document_id": "azure-search-001",
        "title": "Azure AI Search æ¦‚è¦",
        "content": """Azure AI Searchï¼ˆæ—§ Azure Cognitive Searchï¼‰ã¯ã€Microsoft Azure ãŒæä¾›ã™ã‚‹
        ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰ã®ã‚¯ãƒ©ã‚¦ãƒ‰æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚Web ã‚µã‚¤ãƒˆã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€
        ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€é«˜åº¦ãªæ¤œç´¢æ©Ÿèƒ½ã‚’ç°¡å˜ã«çµ±åˆã§ãã¾ã™ã€‚
        ä¸»ãªç‰¹å¾´ã¨ã—ã¦ã€ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã€
        AI ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚""",
        "category": "Azure",
        "subcategory": "AI Services",
        "tags": ["search", "ai", "fulltext", "vector"],
        "author": "Azure Documentation Team",
    },
    {
        "document_id": "azure-search-002",
        "title": "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä»•çµ„ã¿",
        "content": """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’é«˜æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«åŸ‹ã‚è¾¼ã¿ã€
        é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚
        Azure AI Search ã§ã¯ã€HNSWï¼ˆHierarchical Navigable Small Worldï¼‰
        ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ã€é«˜é€Ÿã‹ã¤é«˜ç²¾åº¦ãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
        text-embedding-3-large ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€3072æ¬¡å…ƒã®
        é«˜å“è³ªãªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚""",
        "category": "Azure",
        "subcategory": "Vector Search",
        "tags": ["vector", "embedding", "hnsw", "similarity"],
        "author": "Azure Documentation Team",
    },
    {
        "document_id": "azure-search-003",
        "title": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°",
        "content": """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¯ã€ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ„å‘³çš„ãªé–¢é€£æ€§ã‚’
        ç†è§£ã—ã¦æ¤œç´¢çµæœã‚’æ”¹å–„ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚Azure AI Search ã®
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ©ãƒ³ã‚«ãƒ¼ã¯ã€Microsoft ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€
        æ¤œç´¢çµæœã‚’æ„å‘³çš„ãªé–¢é€£åº¦ã§å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¾ã™ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´ãŒãªãã¦ã‚‚ã€
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã«æœ€ã‚‚é©ã—ãŸçµæœã‚’ä¸Šä½ã«è¡¨ç¤ºã§ãã¾ã™ã€‚""",
        "category": "Azure",
        "subcategory": "Semantic Search",
        "tags": ["semantic", "reranking", "llm", "relevance"],
        "author": "Azure Documentation Team",
    },
    {
        "document_id": "rag-pattern-001",
        "title": "RAG ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³",
        "content": """RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯ã€å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰
        é–¢é€£æƒ…å ±ã‚’å–å¾—ã—ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å›ç­”ç”Ÿæˆã‚’å¼·åŒ–ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚
        Classic RAG ã§ã¯ã€å˜ä¸€ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ LLM ã«æ¸¡ã—ã¾ã™ã€‚
        Agentic RAG ã§ã¯ã€è¤‡é›‘ãªã‚¯ã‚¨ãƒªã‚’è‡ªå‹•çš„ã«åˆ†è§£ã—ã€
        è¤‡æ•°ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦ã€ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚""",
        "category": "Architecture",
        "subcategory": "RAG",
        "tags": ["rag", "llm", "retrieval", "generation"],
        "author": "Solution Architecture Team",
    },
    {
        "document_id": "security-001",
        "title": "Azure AI Search ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
        "content": """Azure AI Search ã§ã¯ã€è¤‡æ•°ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
        èªè¨¼ã«ã¯ã€Managed Identity ã‚’ä½¿ç”¨ã—ãŸ RBAC èªè¨¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
        API ã‚­ãƒ¼èªè¨¼ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ãŒã€æœ¬ç•ªç’°å¢ƒã§ã¯ Managed Identity ãŒ
        ã‚ˆã‚Šã‚»ã‚­ãƒ¥ã‚¢ã§ã™ã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã—ã¦ã€
        Private Endpoint ã‚’ä½¿ç”¨ã—ã¦ã€VNet å†…ã‹ã‚‰ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã§ãã¾ã™ã€‚
        ãƒ‡ãƒ¼ã‚¿ã¯ä¿å­˜æ™‚ã¨è»¢é€æ™‚ã«æš—å·åŒ–ã•ã‚Œã¾ã™ã€‚""",
        "category": "Security",
        "subcategory": "Authentication",
        "tags": ["security", "rbac", "managed-identity", "private-endpoint"],
        "author": "Security Team",
    },
]


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    
    Args:
        text: åˆ†å‰²å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—æ•°ï¼‰
    
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    text = " ".join(text.split())  # ç©ºç™½æ­£è¦åŒ–
    
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # æ–‡ã®å¢ƒç•Œã§åˆ†å‰²ï¼ˆå¥ç‚¹ã‚’æ¢ã™ï¼‰
        if end < len(text):
            boundary = text.rfind("ã€‚", start, end)
            if boundary > start:
                end = boundary + 1
        
        chunks.append(text[start:end].strip())
        start = end - overlap
    
    return chunks


def main():
    """ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("=" * 60)
    print("Azure AI Search RAG Toolkit - Batch Upload Demo")
    print("=" * 60)
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = AzureSearchClient()
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆæ—¢å­˜ã®å ´åˆã¯æ›´æ–°ï¼‰
    schema_path = Path(__file__).parent.parent / "schemas" / "index_schema.json"
    print(f"\nğŸ“‹ Creating/updating index from: {schema_path}")
    
    try:
        client.create_or_update_index(str(schema_path))
        print("âœ… Index ready")
    except Exception as e:
        print(f"âš ï¸  Index operation: {e}")
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™
    print(f"\nğŸ“„ Preparing {len(SAMPLE_DOCUMENTS)} documents...")
    
    documents = []
    for doc_data in SAMPLE_DOCUMENTS:
        # ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
        chunks = chunk_text(doc_data["content"])
        
        for idx, chunk in enumerate(chunks):
            doc = create_document(
                document_id=doc_data["document_id"],
                title=doc_data["title"],
                content=doc_data["content"],
                chunk=chunk,
                chunk_index=idx,
                category=doc_data.get("category"),
                subcategory=doc_data.get("subcategory"),
                tags=doc_data.get("tags", []),
                author=doc_data.get("author"),
                created_date=datetime.now(),
                language="ja",
                confidentiality_level="internal",
            )
            documents.append(doc)
    
    print(f"   Total chunks: {len(documents)}")
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè‡ªå‹•ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    print("\nğŸš€ Uploading documents with auto-vectorization...")
    
    result = client.upload_documents(documents, generate_vectors=True)
    
    print(f"\nğŸ“Š Upload Results:")
    print(f"   Total: {result['total']}")
    print(f"   Succeeded: {result['succeeded']}")
    print(f"   Failed: {result['failed']}")
    
    # ç¢ºèª
    doc_count = client.get_document_count()
    print(f"\nâœ… Index now contains {doc_count} documents")
    
    print("\n" + "=" * 60)
    print("Batch upload completed! Run basic_search.py to test.")


if __name__ == "__main__":
    main()
